{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CUHK 2021-2022 Term 1 \n",
    "#FETC5510 Group 6 Project\n",
    "\n",
    "#Dataset can be found in here (https://www.kaggle.com/c/santander-product-recommendation/data)\n",
    "#The following codes are used to build a product recommendation system based on the customer historical data.\n",
    "#The recommendation system involves two filtering approaches: collaborative filtering and demographic filtering\n",
    "#Output: \n",
    "#- Combined demographic-based and memory-based probabilities\n",
    "#- Top 1 recommended product for each customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /usr/local/Cellar/jupyterlab/3.1.14_1/libexec/lib/python3.9/site-packages (21.3.1)\n",
      "Requirement already satisfied: pandas in /usr/local/Cellar/jupyterlab/3.1.14_1/libexec/lib/python3.9/site-packages (1.3.4)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/Cellar/jupyterlab/3.1.14_1/libexec/lib/python3.9/site-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/Cellar/jupyterlab/3.1.14_1/libexec/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/Cellar/jupyterlab/3.1.14_1/libexec/lib/python3.9/site-packages (from pandas) (1.21.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/Cellar/six/1.16.0_2/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n",
      "Requirement already satisfied: sklearn in /usr/local/Cellar/jupyterlab/3.1.14_1/libexec/lib/python3.9/site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/Cellar/jupyterlab/3.1.14_1/libexec/lib/python3.9/site-packages (from sklearn) (1.0.1)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /usr/local/Cellar/jupyterlab/3.1.14_1/libexec/lib/python3.9/site-packages (from scikit-learn->sklearn) (1.7.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/Cellar/jupyterlab/3.1.14_1/libexec/lib/python3.9/site-packages (from scikit-learn->sklearn) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/Cellar/jupyterlab/3.1.14_1/libexec/lib/python3.9/site-packages (from scikit-learn->sklearn) (3.0.0)\n",
      "Requirement already satisfied: numpy>=1.14.6 in /usr/local/Cellar/jupyterlab/3.1.14_1/libexec/lib/python3.9/site-packages (from scikit-learn->sklearn) (1.21.3)\n",
      "Requirement already satisfied: psutil in /usr/local/Cellar/jupyterlab/3.1.14_1/libexec/lib/python3.9/site-packages (5.8.0)\n",
      "Requirement already satisfied: prettytable in /usr/local/Cellar/jupyterlab/3.1.14_1/libexec/lib/python3.9/site-packages (2.2.1)\n",
      "Requirement already satisfied: wcwidth in /usr/local/Cellar/jupyterlab/3.1.14_1/libexec/lib/python3.9/site-packages (from prettytable) (0.2.5)\n"
     ]
    }
   ],
   "source": [
    "#Step 0: Module installing before building predictive modeling\n",
    "\n",
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade pip\n",
    "!{sys.executable} -m pip install pandas\n",
    "!{sys.executable} -m pip install sklearn\n",
    "!{sys.executable} -m pip install psutil\n",
    "!{sys.executable} -m pip install prettytable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 0: Module importing before building predictive modeling\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import datetime\n",
    "from operator import sub\n",
    "from sklearn import preprocessing, ensemble, metrics\n",
    "import os\n",
    "import gc\n",
    "import psutil\n",
    "import math\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from collections import defaultdict\n",
    "from scipy.spatial.distance import pdist, wminkowski, squareform\n",
    "from prettytable import PrettyTable \n",
    "\n",
    "pd.options.display.max_rows = 100\n",
    "pd.options.display.max_columns = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/jupyterlab/3.1.14_1/libexec/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3444: DtypeWarning: Columns (5,8,11,15) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "/usr/local/Cellar/jupyterlab/3.1.14_1/libexec/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3444: DtypeWarning: Columns (15) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "#Step 0: Data importing\n",
    "#Please note that there should have two set of datasets in the directory before running this code\n",
    "\n",
    "path = '../FETC5510/'\n",
    "traindat = pd.read_csv(path + 'training set.csv', low_memory = True)\n",
    "testdat = pd.read_csv(path + 'testing set.csv', low_memory = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Record in Training Set:  13647309\n",
      "Number of Record in Testing Set:  929615\n"
     ]
    }
   ],
   "source": [
    "#[Other than modeling] Check the number of records in training set and testing set\n",
    "\n",
    "print(\"{} {}\".format(\"Number of Record in Training Set: \",len(traindat)))\n",
    "print(\"{} {}\".format(\"Number of Record in Testing Set: \",len(testdat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2015-01-28    625457\n",
       "2015-02-28    627394\n",
       "2015-03-28    629209\n",
       "2015-04-28    630367\n",
       "2015-05-28    631957\n",
       "2015-06-28    632110\n",
       "2015-07-28    829817\n",
       "2015-08-28    843201\n",
       "2015-09-28    865440\n",
       "2015-10-28    892251\n",
       "2015-11-28    906109\n",
       "2015-12-28    912021\n",
       "2016-01-28    916269\n",
       "2016-02-28    920904\n",
       "2016-03-28    925076\n",
       "2016-04-28    928274\n",
       "2016-05-28    931453\n",
       "Name: fecha_dato, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#[Other than modeling] Summarize the counting of records in training set by date\n",
    "\n",
    "traindat['fecha_dato'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2016-06-28    929615\n",
       "Name: fecha_dato, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#[Other than modeling] Summarize the counting of records in testing set by date\n",
    "\n",
    "testdat['fecha_dato'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1: Data cleaning - Dropping columns deemed surplus to requirements\n",
    "\n",
    "#Define columns to be used for modeling (i.e. demographic data and product data)\n",
    "\n",
    "#Columns of Demographic Data\n",
    "demographic_cols = ['fecha_dato',\n",
    " 'ncodpers','ind_empleado','pais_residencia','sexo','age','fecha_alta','ind_nuevo','antiguedad','indrel',\n",
    " 'indrel_1mes','tiprel_1mes','indresi','indext','canal_entrada','indfall',\n",
    " 'tipodom','cod_prov','ind_actividad_cliente','renta','segmento']\n",
    "\n",
    "#Columns which no longer required\n",
    "notuse = [\"ult_fec_cli_1t\",\"nomprov\"]\n",
    "\n",
    "#Columns of Product\n",
    "product_col = [\n",
    " 'ind_ahor_fin_ult1','ind_aval_fin_ult1','ind_cco_fin_ult1','ind_cder_fin_ult1','ind_cno_fin_ult1','ind_ctju_fin_ult1',\n",
    " 'ind_ctma_fin_ult1','ind_ctop_fin_ult1','ind_ctpp_fin_ult1','ind_deco_fin_ult1','ind_deme_fin_ult1',\n",
    " 'ind_dela_fin_ult1','ind_ecue_fin_ult1','ind_fond_fin_ult1','ind_hip_fin_ult1','ind_plan_fin_ult1',\n",
    " 'ind_pres_fin_ult1','ind_reca_fin_ult1','ind_tjcr_fin_ult1','ind_valo_fin_ult1','ind_viv_fin_ult1','ind_nomina_ult1',\n",
    " 'ind_nom_pens_ult1','ind_recibo_ult1']\n",
    "\n",
    "#Combine demographic columns and product columns and store them into a variable\n",
    "train_cols = demographic_cols + product_col\n",
    "\n",
    "#Create trimmed datasets (only containing necessary columns)\n",
    "traindat = traindat.filter(train_cols)\n",
    "testdat  = testdat.filter(train_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fecha_dato                     0\n",
       "ncodpers                       0\n",
       "ind_empleado               27734\n",
       "pais_residencia            27734\n",
       "sexo                       27804\n",
       "age                            0\n",
       "fecha_alta                 27734\n",
       "ind_nuevo                  27734\n",
       "antiguedad                     0\n",
       "indrel                     27734\n",
       "indrel_1mes               149781\n",
       "tiprel_1mes               149781\n",
       "indresi                    27734\n",
       "indext                     27734\n",
       "canal_entrada             186126\n",
       "indfall                    27734\n",
       "tipodom                    27735\n",
       "cod_prov                   93591\n",
       "ind_actividad_cliente      27734\n",
       "renta                    2794375\n",
       "segmento                  189368\n",
       "ind_ahor_fin_ult1              0\n",
       "ind_aval_fin_ult1              0\n",
       "ind_cco_fin_ult1               0\n",
       "ind_cder_fin_ult1              0\n",
       "ind_cno_fin_ult1               0\n",
       "ind_ctju_fin_ult1              0\n",
       "ind_ctma_fin_ult1              0\n",
       "ind_ctop_fin_ult1              0\n",
       "ind_ctpp_fin_ult1              0\n",
       "ind_deco_fin_ult1              0\n",
       "ind_deme_fin_ult1              0\n",
       "ind_dela_fin_ult1              0\n",
       "ind_ecue_fin_ult1              0\n",
       "ind_fond_fin_ult1              0\n",
       "ind_hip_fin_ult1               0\n",
       "ind_plan_fin_ult1              0\n",
       "ind_pres_fin_ult1              0\n",
       "ind_reca_fin_ult1              0\n",
       "ind_tjcr_fin_ult1              0\n",
       "ind_valo_fin_ult1              0\n",
       "ind_viv_fin_ult1               0\n",
       "ind_nomina_ult1            16063\n",
       "ind_nom_pens_ult1          16063\n",
       "ind_recibo_ult1                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#[Other than modeling] Identify the columns with missing data\n",
    "\n",
    "traindat.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1: Data cleaning - Handling with missing value\n",
    "#1. Factor variables: either impute the most common factor level or set to a new 'missing' level\n",
    "#2. Numerical variables: set the missing value equal to the average for each province\n",
    "#3. Product variables: set to 0\n",
    "\n",
    "traindat.age = pd.to_numeric(traindat.age, errors='coerce')\n",
    "traindat.renta = pd.to_numeric(traindat.renta, errors='coerce')\n",
    "traindat.antiguedad = pd.to_numeric(traindat.antiguedad, errors='coerce')\n",
    "\n",
    "traindat.loc[traindat['ind_empleado'].isnull(),'ind_empleado'] = 'N'\n",
    "traindat.loc[traindat['pais_residencia'].isnull(),'pais_residencia'] = 'ES'\n",
    "traindat.loc[traindat['sexo'].isnull(),'sexo'] = 'V'\n",
    "traindat.fecha_alta = traindat.fecha_alta.astype('datetime64[ns]')\n",
    "traindat.loc[traindat['fecha_alta'].isnull(), 'fecha_alta'] = pd.Timestamp(2011,9,1)\n",
    "traindat.loc[traindat['ind_nuevo'].isnull(), 'ind_nuevo'] = 0\n",
    "traindat.loc[traindat['indrel'].isnull(), 'indrel'] = 1\n",
    "traindat.indrel_1mes = traindat.indrel_1mes.astype('str').str.slice(0,1)\n",
    "traindat.loc[traindat['indrel_1mes'].isnull(), 'indrel_1mes'] = '1'\n",
    "traindat.loc[traindat['tiprel_1mes'].isnull(), 'tiprel_1mes'] = 'I'\n",
    "traindat.loc[traindat['indresi'].isnull(), 'indresi'] = 'S'\n",
    "traindat.loc[traindat['indext'].isnull(), 'indext'] = 'N'\n",
    "traindat.loc[traindat['canal_entrada'].isnull(), 'canal_entrada'] = 'MIS'\n",
    "traindat.loc[traindat['indfall'].isnull(), 'indfall'] = 'N'\n",
    "traindat.loc[traindat['tipodom'].isnull(), 'tipodom'] = 0.0\n",
    "traindat.loc[traindat['cod_prov'].isnull(), 'cod_prov'] = 28.0\n",
    "traindat.loc[traindat['ind_actividad_cliente'].isnull(), 'ind_actividad_cliente'] = 0.0\n",
    "traindat[\"renta\"] = traindat[['renta','cod_prov']].groupby(\"cod_prov\").transform(lambda x: x.fillna(x.mean())) #Replace renta with provincial mean\n",
    "traindat[\"age\"] = traindat[['age','cod_prov']].groupby(\"cod_prov\").transform(lambda x: x.fillna(x.mean())) #Replace age with provincial mean\n",
    "traindat[\"antiguedad\"] = traindat[['antiguedad','cod_prov']].groupby(\"cod_prov\").transform(lambda x: x.fillna(x.mean())) #Replace antiguedad with provincial mean\n",
    "traindat.loc[traindat['segmento'].isnull(), 'segmento'] = '02 - PARTICULARES'\n",
    "traindat.loc[traindat['ind_nomina_ult1'].isnull(), 'ind_nomina_ult1'] = 0\n",
    "traindat.loc[traindat['ind_nom_pens_ult1'].isnull(), 'ind_nom_pens_ult1'] = 0\n",
    "\n",
    "#Impute test data\n",
    "\n",
    "testdat.age = pd.to_numeric(testdat.age, errors='coerce')\n",
    "testdat.antiguedad = pd.to_numeric(testdat.antiguedad, errors='coerce')\n",
    "testdat.renta = pd.to_numeric(testdat.renta, errors='coerce')\n",
    "\n",
    "testdat.loc[testdat['sexo'].isnull(),'sexo'] = 'V'\n",
    "testdat.indrel_1mes = testdat.indrel_1mes.astype('str').str.slice(0,1)\n",
    "testdat.loc[testdat['indrel_1mes'].isnull(), 'indrel_1mes'] = '1'\n",
    "testdat.loc[testdat['tiprel_1mes'].isnull(), 'tiprel_1mes'] = 'I'\n",
    "testdat.loc[testdat['canal_entrada'].isnull(), 'canal_entrada'] = 'MIS'\n",
    "testdat.loc[testdat['cod_prov'].isnull(), 'cod_prov'] = 28.0\n",
    "testdat.loc[testdat['segmento'].isnull(), 'segmento'] = '02 - PARTICULARES'\n",
    "testdat[\"renta\"] = testdat[['renta','cod_prov']].groupby(\"cod_prov\").transform(lambda x: x.fillna(x.mean())) #Replace renta with provincial mean\n",
    "testdat[\"age\"] = testdat[['age','cod_prov']].groupby(\"cod_prov\").transform(lambda x: x.fillna(x.mean())) #Replace age with provincial mean\n",
    "testdat[\"antiguedad\"] = testdat[['antiguedad','cod_prov']].groupby(\"cod_prov\").transform(lambda x: x.fillna(x.mean())) #Replace antiguedad with provincial mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fecha_dato               0\n",
       "ncodpers                 0\n",
       "ind_empleado             0\n",
       "pais_residencia          0\n",
       "sexo                     0\n",
       "age                      0\n",
       "fecha_alta               0\n",
       "ind_nuevo                0\n",
       "antiguedad               0\n",
       "indrel                   0\n",
       "indrel_1mes              0\n",
       "tiprel_1mes              0\n",
       "indresi                  0\n",
       "indext                   0\n",
       "canal_entrada            0\n",
       "indfall                  0\n",
       "tipodom                  0\n",
       "cod_prov                 0\n",
       "ind_actividad_cliente    0\n",
       "renta                    0\n",
       "segmento                 0\n",
       "ind_ahor_fin_ult1        0\n",
       "ind_aval_fin_ult1        0\n",
       "ind_cco_fin_ult1         0\n",
       "ind_cder_fin_ult1        0\n",
       "ind_cno_fin_ult1         0\n",
       "ind_ctju_fin_ult1        0\n",
       "ind_ctma_fin_ult1        0\n",
       "ind_ctop_fin_ult1        0\n",
       "ind_ctpp_fin_ult1        0\n",
       "ind_deco_fin_ult1        0\n",
       "ind_deme_fin_ult1        0\n",
       "ind_dela_fin_ult1        0\n",
       "ind_ecue_fin_ult1        0\n",
       "ind_fond_fin_ult1        0\n",
       "ind_hip_fin_ult1         0\n",
       "ind_plan_fin_ult1        0\n",
       "ind_pres_fin_ult1        0\n",
       "ind_reca_fin_ult1        0\n",
       "ind_tjcr_fin_ult1        0\n",
       "ind_valo_fin_ult1        0\n",
       "ind_viv_fin_ult1         0\n",
       "ind_nomina_ult1          0\n",
       "ind_nom_pens_ult1        0\n",
       "ind_recibo_ult1          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#[Other than modeling] Check to make sure all missing data has been filled\n",
    "\n",
    "traindat.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0p/wklr8kqj0h52vzxckdpz9xhc0000gn/T/ipykernel_716/3095308280.py:21: FutureWarning: casting datetime64[ns] values to int64 with .astype(...) is deprecated and will raise in a future version. Use .view(...) instead.\n",
      "  traindat['fecha_alta'] = pd.cut(traindat.fecha_alta.astype(np.int64)//10**9,\n",
      "/var/folders/0p/wklr8kqj0h52vzxckdpz9xhc0000gn/T/ipykernel_716/3095308280.py:22: FutureWarning: casting datetime64[ns] values to int64 with .astype(...) is deprecated and will raise in a future version. Use .view(...) instead.\n",
      "  bins=bins_dt.astype(np.int64)//10**9,\n",
      "/var/folders/0p/wklr8kqj0h52vzxckdpz9xhc0000gn/T/ipykernel_716/3095308280.py:25: FutureWarning: casting datetime64[ns] values to int64 with .astype(...) is deprecated and will raise in a future version. Use .view(...) instead.\n",
      "  testdat['fecha_alta'] = pd.cut(testdat.fecha_alta.astype(np.int64)//10**9,\n",
      "/var/folders/0p/wklr8kqj0h52vzxckdpz9xhc0000gn/T/ipykernel_716/3095308280.py:26: FutureWarning: casting datetime64[ns] values to int64 with .astype(...) is deprecated and will raise in a future version. Use .view(...) instead.\n",
      "  bins=bins_dt.astype(np.int64)//10**9,\n",
      "/var/folders/0p/wklr8kqj0h52vzxckdpz9xhc0000gn/T/ipykernel_716/3095308280.py:55: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  traindat.antiguedad[traindat.antiguedad<0] = 0\n"
     ]
    }
   ],
   "source": [
    "#Step 2: Feature engineering\n",
    "#Bin the continuous variables and mutate product ownership variables\n",
    "\n",
    "#Additional data cleaning\n",
    "#Observation: based on (omitted) EDA, a pre/post 2011 split would make sense for fecha_alta; as credit recovered following the 2008 crash, we may expect to see different user types\n",
    "traindat[\"fecha_alta\"] = traindat[\"fecha_alta\"].astype(\"datetime64\")\n",
    "testdat[\"fecha_alta\"] = testdat[\"fecha_alta\"].astype(\"datetime64\")\n",
    "\n",
    "#Observation: on a log scale, the salary data is broadly normal. We can take low-medium-high bounds using quartiles\n",
    "traindat[\"renta\"] = np.log(traindat[\"renta\"])\n",
    "testdat[\"renta\"] = np.log(testdat[\"renta\"])\n",
    "\n",
    "#Bin the continuous variables\n",
    "bins_dt = pd.date_range('1994-01-01', freq='16Y', periods=3)\n",
    "bins_str = bins_dt.astype(str).values\n",
    "labels = ['({}, {}]'.format(bins_str[i-1], bins_str[i]) for i in range(1, len(bins_str))]\n",
    "\n",
    "traindat['fecha_alta'] = pd.cut(traindat.fecha_alta.astype(np.int64)//10**9,\n",
    "                   bins=bins_dt.astype(np.int64)//10**9,\n",
    "                   labels=labels)\n",
    "\n",
    "testdat['fecha_alta'] = pd.cut(testdat.fecha_alta.astype(np.int64)//10**9,\n",
    "                   bins=bins_dt.astype(np.int64)//10**9,\n",
    "                   labels=labels)\n",
    "\n",
    "\n",
    "bins_renta = [0,np.percentile(traindat.renta, 25),np.percentile(traindat.renta, 75),25]\n",
    "\n",
    "traindat['renta'] = pd.cut(traindat.renta,\n",
    "                   bins=bins_renta)\n",
    "\n",
    "testdat['renta'] = pd.cut(testdat.renta,\n",
    "                   bins=bins_renta)\n",
    "\n",
    "\n",
    "bins_age = [0,25,42,60,1000]\n",
    "labels_age = ['young','middle','older','old']\n",
    "\n",
    "traindat['age'] = pd.cut(traindat.age,\n",
    "                   bins=bins_age,\n",
    "                   labels=labels_age)\n",
    "\n",
    "testdat['age'] = pd.cut(testdat.age,\n",
    "                   bins=bins_age,\n",
    "                   labels=labels_age)\n",
    "\n",
    "\n",
    "bins_anti = [-1,220,300]\n",
    "labels_anti = ['new','old']\n",
    "\n",
    "#Remove negative antiguedad values\n",
    "traindat.antiguedad[traindat.antiguedad<0] = 0\n",
    "\n",
    "traindat['antiguedad'] = pd.cut(traindat.antiguedad,\n",
    "                   bins=bins_anti,\n",
    "                   labels=labels_anti)\n",
    "\n",
    "testdat['antiguedad'] = pd.cut(testdat.antiguedad,\n",
    "                   bins=bins_anti,\n",
    "                   labels=labels_anti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3: Data Subsetting\n",
    "#Only used June 2015 data (i.e. one year before the test data), its associated lagged month and lagged May 2016 as a predictor\n",
    "\n",
    "traindat = traindat[traindat.fecha_dato.isin(['2015-05-28','2015-06-28','2016-05-28'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sort completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0p/wklr8kqj0h52vzxckdpz9xhc0000gn/T/ipykernel_716/20090700.py:14: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  traindat_use = pd.merge(traindat,train_index.drop(merge_drop_cols,1), on=['new','ncodpers'],how='left',suffixes=['','_previous'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merge completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0p/wklr8kqj0h52vzxckdpz9xhc0000gn/T/ipykernel_716/20090700.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  traindat_use[i][traindat_use[i] < 0] = 0\n"
     ]
    }
   ],
   "source": [
    "#In order to join each user with itself in the previous month\n",
    "\n",
    "#First sort data based on key columns\n",
    "traindat = traindat.sort_values(['ncodpers','fecha_dato'],ascending=[True,True]).reset_index(drop=True)\n",
    "print('sort completed')\n",
    "\n",
    "#Then create a new dataset where the index is incremented...\n",
    "traindat['new'] = traindat.index\n",
    "train_index = traindat.copy()\n",
    "train_index['new'] += 1\n",
    "\n",
    "#Then merge the dataset with itself to add each user's purchases in the previous month\n",
    "#Rename these new columns with a '_previous' suffix \n",
    "merge_drop_cols = demographic_cols.copy()\n",
    "merge_drop_cols.remove('ncodpers')\n",
    "traindat_use = pd.merge(traindat,train_index.drop(merge_drop_cols,1), on=['new','ncodpers'],how='left',suffixes=['','_previous'])\n",
    "print('merge completed')\n",
    "\n",
    "#Replace current with (current - previous) to obtain purchase indicators\n",
    "for i in product_col:\n",
    "    traindat_use[i] = traindat_use[i]-traindat_use[i+\"_previous\"]\n",
    "    \n",
    "    #Replace negative values with 0: \n",
    "    #if a user gets rid of a product from month x to month x+1, this registers as no purchase in the evaluation metric, so treat it as no purchase made\n",
    "    traindat_use[i][traindat_use[i] < 0] = 0\n",
    "\n",
    "#Fill in na values created by merge\n",
    "traindat_use[product_col] = traindat_use[product_col].fillna(0)\n",
    "new_product_col = [i + \"_previous\" for i in product_col]\n",
    "traindat_use[new_product_col] = traindat_use[new_product_col].fillna(0)\n",
    "\n",
    "#Delete redundant objects to free up memory\n",
    "del train_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "736"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Add purchase history columns to the testing set to prediction purpose\n",
    "test_col = product_col + ['ncodpers']\n",
    "testdat_use = pd.merge(testdat,traindat[traindat.fecha_dato=='2016-05-28'][test_col],on='ncodpers',how='left',suffixes=['','_previous'])\n",
    "\n",
    "testdat_use.rename(\n",
    "    columns={i:j for i,j in zip(product_col,new_product_col)}, inplace=True\n",
    ")\n",
    "\n",
    "testdat_use[new_product_col] = testdat_use[new_product_col].fillna(0)\n",
    "\n",
    "#Delete redundant objects to free up memory\n",
    "del traindat, testdat\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0p/wklr8kqj0h52vzxckdpz9xhc0000gn/T/ipykernel_716/1191215837.py:32: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  testdat_final_unique = testdat_final.drop('ncodpers',1).drop_duplicates().copy().reset_index(drop=True)\n",
      "/var/folders/0p/wklr8kqj0h52vzxckdpz9xhc0000gn/T/ipykernel_716/1191215837.py:49: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  testdat_demog_final_unique = testdat_demog_final.drop('ncodpers',1).drop_duplicates().copy().reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "#Step 4: De-duplicate the Data\n",
    "\n",
    "#Pull through variables for memory-based CF\n",
    "traindat_purchases = traindat_use[traindat_use.fecha_dato == '2015-06-28'][product_col].copy()\n",
    "traindat_final = traindat_use[traindat_use.fecha_dato == '2015-06-28'][new_product_col].copy()\n",
    "\n",
    "#Pull through variables for demographic-based CF\n",
    "demog_col = ['sexo','age','fecha_alta','ind_nuevo','indrel','indresi','indfall','tipodom','ind_actividad_cliente']\n",
    "traindat_demog_final = traindat_use[traindat_use.fecha_dato == '2015-06-28'][demog_col].copy()\n",
    "\n",
    "#Transform demographic factor variables into binary format\n",
    "sexo_map = {'V': 1,'H': 0}\n",
    "age_map = {'old': 1,'young': 0}\n",
    "fecha_alta_map = {'(1994-12-31, 2010-12-31]': 1,'(2010-12-31, 2026-12-31]': 0}\n",
    "indresi_map = {'S': 1,'N': 0}\n",
    "indfall_map = {'S': 1,'N': 0}\n",
    "\n",
    "traindat_demog_final.loc[traindat_demog_final['age']=='older', 'age'] = 'old'\n",
    "traindat_demog_final.loc[traindat_demog_final['age']=='middle', 'age'] = 'young'\n",
    "traindat_demog_final.sexo = [sexo_map[item] for item in traindat_demog_final.sexo]\n",
    "traindat_demog_final.age = [age_map[item] for item in traindat_demog_final.age]\n",
    "traindat_demog_final.fecha_alta = [fecha_alta_map[item] for item in traindat_demog_final.fecha_alta]\n",
    "traindat_demog_final.indresi = [indresi_map[item] for item in traindat_demog_final.indresi]\n",
    "traindat_demog_final.indfall = [indfall_map[item] for item in traindat_demog_final.indfall]\n",
    "\n",
    "#Collect all the observed combinations of purchase history\n",
    "new_product_col_aug = new_product_col + ['ncodpers']\n",
    "testdat_final = testdat_use[new_product_col_aug].copy()\n",
    "testdat_final_unique = testdat_final.drop('ncodpers',1).drop_duplicates().copy().reset_index(drop=True)\n",
    "\n",
    "#Transform demographic factor variables into binary format\n",
    "demog_col_aug = demog_col + ['ncodpers']\n",
    "testdat_demog_final = testdat_use[demog_col_aug].copy()\n",
    "\n",
    "testdat_demog_final.loc[testdat_demog_final['age']=='older', 'age'] = 'old'\n",
    "testdat_demog_final.loc[testdat_demog_final['age']=='middle', 'age'] = 'young'\n",
    "testdat_demog_final.sexo = [sexo_map[item] for item in testdat_demog_final.sexo]\n",
    "testdat_demog_final.age = [age_map[item] for item in testdat_demog_final.age]\n",
    "testdat_demog_final.fecha_alta = [fecha_alta_map[item] for item in testdat_demog_final.fecha_alta]\n",
    "testdat_demog_final.indresi = [indresi_map[item] for item in testdat_demog_final.indresi]\n",
    "testdat_demog_final.indfall = [indfall_map[item] for item in testdat_demog_final.indfall] \n",
    "\n",
    "testdat_demog_final_unique = testdat_demog_final.drop('ncodpers',1).drop_duplicates().copy().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the training data into 'training' and 'test' sets\n",
    "\n",
    "#Create 80% index\n",
    "traindat_index = np.random.rand(len(traindat_final)) < 0.8\n",
    "\n",
    "#Create traindat_train\n",
    "traindat_train = traindat_final[traindat_index]\n",
    "\n",
    "#Create traindat_test\n",
    "traindat_test = traindat_final[~traindat_index]\n",
    "\n",
    "#Make traindat_test unique\n",
    "traindat_test_unique = traindat_test.drop_duplicates().copy().reset_index(drop=True)\n",
    "\n",
    "#Create traindat_purchases\n",
    "traindat_purchases_train = traindat_purchases[traindat_index]\n",
    "\n",
    "#Create traindat_purchases_test for verification\n",
    "traindat_purchases_test = traindat_purchases[~traindat_index]\n",
    "\n",
    "#Create training ncodpers index\n",
    "traindat_ncodpers = traindat_use[traindat_use.fecha_dato == '2015-06-28'][traindat_index][['fecha_dato','ncodpers']]\n",
    "traindat_test_ncodpers = traindat_use[traindat_use.fecha_dato == '2015-06-28'][~traindat_index][['fecha_dato','ncodpers']]\n",
    "\n",
    "#Repeat for demographic columns\n",
    "#Create traindat_demog_train\n",
    "traindat_demog_train = traindat_demog_final[traindat_index]\n",
    "\n",
    "#Create traindat_demog_test\n",
    "traindat_demog_test = traindat_demog_final[~traindat_index]\n",
    "\n",
    "#Make traindat_demog_test unique\n",
    "traindat_demog_test_unique = traindat_demog_test.drop_duplicates().copy().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 5: Build Demographic-Based and Memory-Based Similarity Matrices\n",
    "\n",
    "predict_product_col = [i + \"_predict\" for i in new_product_col]\n",
    "\n",
    "def probability_calculation(dataset,training,training_purchases,used_columns,metric,test_remap,print_option=False):\n",
    "    #'dataset' takes the unique test data with purchase/demographic history; 'training' are the training data that we calculate distances to\n",
    "    \n",
    "    n = dataset.shape[0]\n",
    "    \n",
    "    for index, row in dataset.iterrows():\n",
    "        if print_option == True:\n",
    "            print(str(index) + '/' + str(n))\n",
    "        row_use = row.to_frame().T\n",
    "        \n",
    "        #Store purchase history for the test users\n",
    "        row_history = row_use[used_columns]\n",
    "        \n",
    "        #Calculate distances between the test point and each training point based on selected binary features\n",
    "        #Use 'manhattan' when data was binary - when weighted against demographics, use Euclidean\n",
    "        distances = metrics.pairwise_distances(row_use,training,metric=metric) + 1e-6\n",
    "        \n",
    "        #Normalise distances: previously used 24-distances, and 1/(1+distances), but the asymptotic behaviour of 1/distances gives the most accurate predictions.\n",
    "        norm_distances = 1/distances\n",
    "        \n",
    "        #Take dot product between distance to training point and training point's purchase history to obtain ownership likelihood matrix\n",
    "        sim = pd.DataFrame(norm_distances.dot(training_purchases)/np.sum(norm_distances),columns = new_product_col)\n",
    "        if(index == 0):\n",
    "            probabilities = sim\n",
    "        else:\n",
    "            probabilities = probabilities.append(sim)\n",
    "    print(\"probabilities calculated\")\n",
    "    \n",
    "    #Reindex users for join\n",
    "    reindexed_output = probabilities.reset_index().drop('index',axis=1).copy()\n",
    "    indexed_unique_test = dataset.reset_index().drop('index',axis=1).copy()\n",
    "    output_unique = indexed_unique_test.join(reindexed_output,rsuffix='_predict')\n",
    "    output_final = pd.merge(test_remap,output_unique,on=used_columns,how='left')\n",
    "    \n",
    "    #Only select relevant products\n",
    "    output_final = output_final.drop(used_columns,1)\n",
    "    output_final.columns = output_final.columns.str.replace(\"_predict\", \"\")\n",
    "    output_final.columns = output_final.columns.str.replace(\"_previous\", \"_predict\")\n",
    "    \n",
    "    #Have all test probabilities - can average and compare with results\n",
    "    return output_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probabilities calculated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0p/wklr8kqj0h52vzxckdpz9xhc0000gn/T/ipykernel_716/3255830827.py:32: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  output_final = output_final.drop(used_columns,1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probabilities calculated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0p/wklr8kqj0h52vzxckdpz9xhc0000gn/T/ipykernel_716/3255830827.py:32: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  output_final = output_final.drop(used_columns,1)\n"
     ]
    }
   ],
   "source": [
    "#Step 6: Combine Demographic-Based and Memory-Based Probabilities\n",
    "\n",
    "#Calculate memory-based similarities\n",
    "probabilities_memory = probability_calculation(traindat_test_unique,traindat_train,traindat_purchases_train,new_product_col,'manhattan',traindat_test)\n",
    "\n",
    "#Calculate demographic-based similarities\n",
    "probabilities_demog = probability_calculation(traindat_demog_test_unique,traindat_demog_train,traindat_purchases_train,demog_col,'manhattan',traindat_demog_test)\n",
    "\n",
    "#Average predictions for a range of mixing probabilities\n",
    "probabilities_avg_95 = 0.95*probabilities_memory + 0.05*probabilities_demog\n",
    "probabilities_avg_90 = 0.9*probabilities_memory + 0.1*probabilities_demog\n",
    "probabilities_avg_85 = 0.85*probabilities_memory + 0.15*probabilities_demog\n",
    "probabilities_avg_80 = 0.8*probabilities_memory + 0.2*probabilities_demog\n",
    "probabilities_avg_75 = 0.75*probabilities_memory + 0.25*probabilities_demog\n",
    "probabilities_avg_70 = 0.7*probabilities_memory + 0.3*probabilities_demog\n",
    "probabilities_avg_65 = 0.65*probabilities_memory + 0.35*probabilities_demog\n",
    "probabilities_avg_60 = 0.6*probabilities_memory + 0.4*probabilities_demog\n",
    "probabilities_avg_55 = 0.55*probabilities_memory + 0.45*probabilities_demog\n",
    "probabilities_avg_50 = 0.5*probabilities_memory + 0.5*probabilities_demog\n",
    "probabilities_avg_45 = 0.45*probabilities_memory + 0.55*probabilities_demog\n",
    "probabilities_avg_40 = 0.4*probabilities_memory + 0.6*probabilities_demog\n",
    "probabilities_avg_35 = 0.35*probabilities_memory + 0.65*probabilities_demog\n",
    "probabilities_avg_30 = 0.3*probabilities_memory + 0.7*probabilities_demog\n",
    "probabilities_avg_25 = 0.25*probabilities_memory + 0.75*probabilities_demog\n",
    "probabilities_avg_20 = 0.2*probabilities_memory + 0.8*probabilities_demog\n",
    "probabilities_avg_15 = 0.15*probabilities_memory + 0.85*probabilities_demog\n",
    "probabilities_avg_10 = 0.1*probabilities_memory + 0.9*probabilities_demog\n",
    "probabilities_avg_5 = 0.05*probabilities_memory + 0.95*probabilities_demog\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_col = [i + \"_predict\" for i in product_col]\n",
    "predict_previous_col = predict_col + new_product_col\n",
    "\n",
    "def purchase_nullifier(probabilities,purchase_history,print_option=False):\n",
    "    #function to 'nullify' any probabilities that would lead to an owned product being predicted\n",
    "\n",
    "    #Join two datasets together\n",
    "    purchase_history = purchase_history.reset_index(drop=True)\n",
    "    joined_data = purchase_history.join(probabilities)\n",
    "    \n",
    "    #Shrink dataset to deal with large-scale data\n",
    "    unique_data = joined_data.drop_duplicates().copy().reset_index(drop=True)\n",
    "    n = unique_data.shape[0]\n",
    "    print(\"data joined\")\n",
    "    \n",
    "    for index,row in unique_data.iterrows():\n",
    "        if print_option == True:\n",
    "            print(str(index) + \"/\" + str(n))\n",
    "        row = row.to_frame().T\n",
    "        #Subset dataframe and rename columns for nullification\n",
    "        row_purchases = row[new_product_col]\n",
    "        row_purchases.columns = row_purchases.columns.str.replace(\"_previous\",\"\")\n",
    "        row_probabilities = row[predict_col]\n",
    "        row_probabilities.columns = row_probabilities.columns.str.replace(\"_predict\",\"\")\n",
    "        prob_norm = (1-row_purchases).multiply(row_probabilities,axis=0)\n",
    "        if(index == 0):\n",
    "            output_norm = prob_norm\n",
    "        else:\n",
    "            output_norm = output_norm.append(prob_norm)\n",
    "    print(\"nullification complete\")\n",
    "    \n",
    "    #Duplicate back up to original dataset\n",
    "    #Add columns to enable merge\n",
    "    output_index = output_norm.reset_index(drop=True)\n",
    "    prob_predict = output_index.join(unique_data)\n",
    "    scaled_predict = pd.merge(joined_data,prob_predict,how='left')\n",
    "    output = scaled_predict[product_col]\n",
    "    output.columns = output.columns.str.replace(\"ult1\",\"ult1_predict\")\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data joined\n",
      "nullification complete\n",
      "data joined\n",
      "nullification complete\n",
      "data joined\n",
      "nullification complete\n",
      "data joined\n",
      "nullification complete\n",
      "data joined\n",
      "nullification complete\n",
      "data joined\n",
      "nullification complete\n",
      "data joined\n",
      "nullification complete\n",
      "data joined\n",
      "nullification complete\n",
      "data joined\n",
      "nullification complete\n",
      "data joined\n",
      "nullification complete\n",
      "data joined\n",
      "nullification complete\n",
      "data joined\n",
      "nullification complete\n",
      "data joined\n",
      "nullification complete\n",
      "data joined\n",
      "nullification complete\n",
      "data joined\n",
      "nullification complete\n",
      "data joined\n",
      "nullification complete\n",
      "data joined\n",
      "nullification complete\n",
      "data joined\n",
      "nullification complete\n",
      "data joined\n",
      "nullification complete\n",
      "data joined\n",
      "nullification complete\n",
      "data joined\n",
      "nullification complete\n"
     ]
    }
   ],
   "source": [
    "nulled_probabilities_100 = purchase_nullifier(probabilities_memory,traindat_test)\n",
    "nulled_probabilities_95 = purchase_nullifier(probabilities_avg_95,traindat_test)\n",
    "nulled_probabilities_90 = purchase_nullifier(probabilities_avg_90,traindat_test)\n",
    "nulled_probabilities_85 = purchase_nullifier(probabilities_avg_85,traindat_test)\n",
    "nulled_probabilities_80 = purchase_nullifier(probabilities_avg_80,traindat_test)\n",
    "nulled_probabilities_75 = purchase_nullifier(probabilities_avg_75,traindat_test)\n",
    "nulled_probabilities_70 = purchase_nullifier(probabilities_avg_70,traindat_test)\n",
    "nulled_probabilities_65 = purchase_nullifier(probabilities_avg_65,traindat_test)\n",
    "nulled_probabilities_60 = purchase_nullifier(probabilities_avg_60,traindat_test)\n",
    "nulled_probabilities_55 = purchase_nullifier(probabilities_avg_55,traindat_test)\n",
    "nulled_probabilities_50 = purchase_nullifier(probabilities_avg_50,traindat_test)\n",
    "nulled_probabilities_45 = purchase_nullifier(probabilities_avg_45,traindat_test)\n",
    "nulled_probabilities_40 = purchase_nullifier(probabilities_avg_40,traindat_test)\n",
    "nulled_probabilities_35 = purchase_nullifier(probabilities_avg_35,traindat_test)\n",
    "nulled_probabilities_30 = purchase_nullifier(probabilities_avg_30,traindat_test)\n",
    "nulled_probabilities_25 = purchase_nullifier(probabilities_avg_25,traindat_test)\n",
    "nulled_probabilities_20 = purchase_nullifier(probabilities_avg_20,traindat_test)\n",
    "nulled_probabilities_15 = purchase_nullifier(probabilities_avg_15,traindat_test)\n",
    "nulled_probabilities_10 = purchase_nullifier(probabilities_avg_10,traindat_test)\n",
    "nulled_probabilities_5 = purchase_nullifier(probabilities_avg_5,traindat_test)\n",
    "nulled_probabilities_0 = purchase_nullifier(probabilities_demog,traindat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 7: Derive Recommendations\n",
    "\n",
    "def probabilities_to_predictions(probabilities,ncodpers,print_option=False):\n",
    "# ncodpers is a dataframe with two columns: fecha_dato and ncodpers (corresponding to probabilities order)    \n",
    "    \n",
    "    #Make probabilities unique to speed upc calculations\n",
    "    unique_probabilities = probabilities.drop_duplicates().copy().reset_index(drop=True)\n",
    "    print(unique_probabilities.shape)\n",
    "    \n",
    "    n = unique_probabilities.shape[0]\n",
    "    \n",
    "    for index, row in unique_probabilities.iterrows():\n",
    "        if print_option == True:\n",
    "            print(str(index) + '/' + str(n))\n",
    "        row_use = row.to_frame().T\n",
    "        \n",
    "        #Rank list of product recommendations\n",
    "        arank = row_use.apply(np.argsort, axis=1)\n",
    "        ranked_cols = row_use.columns.to_series()[np. reshape(arank.values[:,::-1][:,:7],-1)] #Updated\n",
    "        \n",
    "        new_frame = pd.DataFrame(ranked_cols)\n",
    "        \n",
    "        #Concatenate all 7 predictions\n",
    "        recoms = new_frame[0]\n",
    "        recoms_final = recoms.str.replace('_predict', '', regex=True)\n",
    "        if(index == 0):\n",
    "            predictions = recoms_final\n",
    "        else:\n",
    "            predictions = predictions.append(recoms_final)\n",
    "    \n",
    "    #Merge predictions back to initial indices for full dataset\n",
    "    mapped_predictions = predictions.to_frame().rename(columns={0:'added_products'}).reset_index(drop=True)\n",
    "    output_unique = mapped_predictions.join(unique_probabilities)\n",
    "    output_final = pd.merge(probabilities,output_unique,on=predict_col,how='left')\n",
    "    \n",
    "    #Add ncodpers for final submission file\n",
    "    no_index_ncodpers = ncodpers.copy().reset_index(drop=True)\n",
    "    output_ncodpers = no_index_ncodpers.join(output_final['added_products']).drop('fecha_dato',axis=1)\n",
    "    return output_ncodpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2958, 24)\n",
      "(6635, 24)\n",
      "(6635, 24)\n",
      "(6635, 24)\n",
      "(6635, 24)\n",
      "(6635, 24)\n",
      "(6635, 24)\n",
      "(6635, 24)\n",
      "(6635, 24)\n",
      "(6635, 24)\n",
      "(6635, 24)\n",
      "(6635, 24)\n",
      "(6635, 24)\n",
      "(6635, 24)\n",
      "(6635, 24)\n",
      "(6635, 24)\n",
      "(6635, 24)\n",
      "(6635, 24)\n",
      "(6635, 24)\n",
      "(6635, 24)\n",
      "(6620, 24)\n"
     ]
    }
   ],
   "source": [
    "predictions_output_100 = probabilities_to_predictions(nulled_probabilities_100,traindat_test_ncodpers)\n",
    "predictions_output_95 = probabilities_to_predictions(nulled_probabilities_95,traindat_test_ncodpers)\n",
    "predictions_output_90 = probabilities_to_predictions(nulled_probabilities_90,traindat_test_ncodpers)\n",
    "predictions_output_85 = probabilities_to_predictions(nulled_probabilities_85,traindat_test_ncodpers)\n",
    "predictions_output_80 = probabilities_to_predictions(nulled_probabilities_80,traindat_test_ncodpers)\n",
    "predictions_output_75 = probabilities_to_predictions(nulled_probabilities_75,traindat_test_ncodpers)\n",
    "predictions_output_70 = probabilities_to_predictions(nulled_probabilities_70,traindat_test_ncodpers)\n",
    "predictions_output_65 = probabilities_to_predictions(nulled_probabilities_65,traindat_test_ncodpers)\n",
    "predictions_output_60 = probabilities_to_predictions(nulled_probabilities_60,traindat_test_ncodpers)\n",
    "predictions_output_55 = probabilities_to_predictions(nulled_probabilities_55,traindat_test_ncodpers)\n",
    "predictions_output_50 = probabilities_to_predictions(nulled_probabilities_50,traindat_test_ncodpers)\n",
    "predictions_output_45 = probabilities_to_predictions(nulled_probabilities_45,traindat_test_ncodpers)\n",
    "predictions_output_40 = probabilities_to_predictions(nulled_probabilities_40,traindat_test_ncodpers)\n",
    "predictions_output_35 = probabilities_to_predictions(nulled_probabilities_35,traindat_test_ncodpers)\n",
    "predictions_output_30 = probabilities_to_predictions(nulled_probabilities_30,traindat_test_ncodpers)\n",
    "predictions_output_25 = probabilities_to_predictions(nulled_probabilities_25,traindat_test_ncodpers)\n",
    "predictions_output_20 = probabilities_to_predictions(nulled_probabilities_20,traindat_test_ncodpers)\n",
    "predictions_output_15 = probabilities_to_predictions(nulled_probabilities_15,traindat_test_ncodpers)\n",
    "predictions_output_10 = probabilities_to_predictions(nulled_probabilities_10,traindat_test_ncodpers)\n",
    "predictions_output_5 = probabilities_to_predictions(nulled_probabilities_5,traindat_test_ncodpers)\n",
    "predictions_output_0 = probabilities_to_predictions(nulled_probabilities_0,traindat_test_ncodpers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation metric\n",
    "#To ascertain the performance of each model (i.e. each mixing probability)\n",
    "\n",
    "evaluation_col = product_col + ['added_products']\n",
    "\n",
    "def evaluation_metric(predictions,reality,print_option=False):\n",
    "    #Predictions is a list of the top seven purchase likelihood indicators; reality is the actual purchases\n",
    "    reality = reality.reset_index(drop=True)\n",
    "    \n",
    "    #Find unique combinations to speed up function: merge data, group_by, count (then multiply results at the end)\n",
    "    reality['added_products'] = predictions['added_products']\n",
    "    data_unique = reality.drop_duplicates().copy().reset_index(drop=True)\n",
    "    predictions_unique = data_unique['added_products'].to_frame()\n",
    "    reality_unique = data_unique.drop('added_products',1)\n",
    "    n = predictions_unique.shape[0]\n",
    "    for index, row in predictions_unique.iterrows():\n",
    "        if print_option == True:\n",
    "            print(str(index) + '/' + str(n))\n",
    "        prediction_use = row.to_frame().T['added_products'].str.split(' ',expand=True).T\n",
    "        prediction_use = prediction_use.rename(columns={list(prediction_use)[0]:'predict_products'})\n",
    "\n",
    "        #Only take top 7 products purchased\n",
    "        reality_use = reality_unique.iloc[index].to_frame()\n",
    "        reality_use = reality_use.rename(columns={list(reality_use)[0]:'added_products'})\n",
    "        reality_use['product_name'] = reality_use.index\n",
    "        reality_use = reality_use[reality_use.added_products==1]\n",
    "        reality_use['ind'] = 1\n",
    "\n",
    "        if reality_use.empty:\n",
    "            P = [0]\n",
    "        else:\n",
    "            #Calculate precision @7: what average proportion of our predictions are purchased?\n",
    "            P = [precision_at_k(prediction_use,reality_use)]\n",
    "        if index == 0:\n",
    "            eval_sum = P\n",
    "        else:\n",
    "            eval_sum.extend(P)\n",
    "    \n",
    "    #Duplicate back up\n",
    "    print('precisions calculated')\n",
    "    data_unique['precision'] = eval_sum\n",
    "    reality_final = pd.merge(reality,data_unique,on=evaluation_col,how='left')\n",
    "    U = predictions.shape[0]\n",
    "    output = sum(reality_final.precision)/U\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(prediction,reality):\n",
    "    #'prediction' is a data frame with a column 'predict_products' containing our 7 predictions\n",
    "    #'reality' is a data frame with a column 'added_products' containing any products purchased (always non-empty)\n",
    "    summand = min(prediction.shape[0],7)\n",
    "    sum_prec = 0\n",
    "    \n",
    "    for k in range(summand):\n",
    "        #Calculate precision at k (careful with 0 index)\n",
    "        top_k_predictions = prediction.head(k+1)\n",
    "        \n",
    "        #Join additions to reduced predictions\n",
    "        add_vs_pred = pd.merge(reality,top_k_predictions,left_on='product_name',right_on='predict_products',how='inner')\n",
    "        sum_prec = sum_prec + sum(add_vs_pred.ind)/top_k_predictions.shape[0]\n",
    "    \n",
    "    denom = min(reality.shape[0],7)\n",
    "    \n",
    "    #Defined as in evaluation_metric function 'reality_use' is always non-empty\n",
    "    output = sum_prec/denom\n",
    "    \n",
    "    return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0p/wklr8kqj0h52vzxckdpz9xhc0000gn/T/ipykernel_716/2777252488.py:10: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  reality_unique = data_unique.drop('added_products',1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precisions calculated\n",
      "precisions calculated\n",
      "precisions calculated\n",
      "precisions calculated\n",
      "precisions calculated\n",
      "precisions calculated\n",
      "precisions calculated\n",
      "precisions calculated\n",
      "precisions calculated\n",
      "precisions calculated\n",
      "precisions calculated\n",
      "precisions calculated\n",
      "precisions calculated\n",
      "precisions calculated\n",
      "precisions calculated\n",
      "precisions calculated\n",
      "precisions calculated\n",
      "precisions calculated\n",
      "precisions calculated\n",
      "precisions calculated\n",
      "precisions calculated\n"
     ]
    }
   ],
   "source": [
    "evaluation_100 = evaluation_metric(predictions_output_100,traindat_purchases_test)\n",
    "evaluation_95 = evaluation_metric(predictions_output_95,traindat_purchases_test)\n",
    "evaluation_90 = evaluation_metric(predictions_output_90,traindat_purchases_test)\n",
    "evaluation_85 = evaluation_metric(predictions_output_85,traindat_purchases_test)\n",
    "evaluation_80 = evaluation_metric(predictions_output_80,traindat_purchases_test)\n",
    "evaluation_75 = evaluation_metric(predictions_output_75,traindat_purchases_test)\n",
    "evaluation_70 = evaluation_metric(predictions_output_70,traindat_purchases_test)\n",
    "evaluation_65 = evaluation_metric(predictions_output_65,traindat_purchases_test)\n",
    "evaluation_60 = evaluation_metric(predictions_output_60,traindat_purchases_test)\n",
    "evaluation_55 = evaluation_metric(predictions_output_55,traindat_purchases_test)\n",
    "evaluation_50 = evaluation_metric(predictions_output_50,traindat_purchases_test)\n",
    "evaluation_45 = evaluation_metric(predictions_output_45,traindat_purchases_test)\n",
    "evaluation_40 = evaluation_metric(predictions_output_40,traindat_purchases_test)\n",
    "evaluation_35 = evaluation_metric(predictions_output_35,traindat_purchases_test)\n",
    "evaluation_30 = evaluation_metric(predictions_output_30,traindat_purchases_test)\n",
    "evaluation_25 = evaluation_metric(predictions_output_25,traindat_purchases_test)\n",
    "evaluation_20 = evaluation_metric(predictions_output_20,traindat_purchases_test)\n",
    "evaluation_15 = evaluation_metric(predictions_output_15,traindat_purchases_test)\n",
    "evaluation_10 = evaluation_metric(predictions_output_10,traindat_purchases_test)\n",
    "evaluation_5 = evaluation_metric(predictions_output_5,traindat_purchases_test)\n",
    "evaluation_0 = evaluation_metric(predictions_output_0,traindat_purchases_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------------------+\n",
      "|      Model       |   Mixing Probability  |\n",
      "+------------------+-----------------------+\n",
      "|    All memory    |  0.003316739757417707 |\n",
      "|    95% memory    |  0.004741021690174234 |\n",
      "|    90% memory    |  0.004719292007427603 |\n",
      "|    85% memory    |  0.004726535235009812 |\n",
      "|    80% memory    |  0.004284039877260215 |\n",
      "|    75% memory    |  0.004172099087353322 |\n",
      "|    70% memory    |  0.004123371919982086 |\n",
      "|    65% memory    |  0.004128639721860057 |\n",
      "|    60% memory    |  0.004742338640643728 |\n",
      "|    55% memory    |  0.004638299553553793 |\n",
      "|    50% memory    |  0.004631714801206329 |\n",
      "|    45% memory    |  0.004658712285830933 |\n",
      "|    40% memory    |  0.004617886821276655 |\n",
      "|    35% memory    |  0.004587596960478318 |\n",
      "|    30% memory    |  0.004542820644515561 |\n",
      "|    25% memory    |  0.004541503694046069 |\n",
      "|    20% memory    |  0.004719950482662351 |\n",
      "|    15% memory    |  0.004538211317872336 |\n",
      "|    10% memory    | 0.0042122660766728505 |\n",
      "|    5% memory     | 0.0044203442508527245 |\n",
      "| All demographics |  0.004340668747448406 |\n",
      "+------------------+-----------------------+\n",
      "Maximum Mixing Probability:  0.004742338640643728  ( 60% memory )\n"
     ]
    }
   ],
   "source": [
    "#[Other than modeling] Check the optimal weight between memory based and demographic based\n",
    "\n",
    "#Specify the Column Names while initializing the Table \n",
    "myTable = PrettyTable([\"Model\", \"Mixing Probability\"]) \n",
    "\n",
    "model_list = [\"All memory\",\"95% memory\",\"90% memory\",\"85% memory\", \"80% memory\",\"75% memory\", \"70% memory\",\n",
    "             \"65% memory\",\"60% memory\",\"55% memory\",\"50% memory\",\"45% memory\",\"40% memory\",\"35% memory\",\n",
    "             \"30% memory\",\"25% memory\",\"20% memory\",\"15% memory\",\"10% memory\",\"5% memory\",\"All demographics\"]\n",
    "\n",
    "memory_list = [evaluation_100, evaluation_95, evaluation_90, evaluation_85, evaluation_80, evaluation_75, \n",
    "              evaluation_70, evaluation_65, evaluation_60, evaluation_55, evaluation_50, evaluation_45, \n",
    "              evaluation_40, evaluation_35, evaluation_30, evaluation_25, evaluation_20, evaluation_15,\n",
    "              evaluation_10, evaluation_5,evaluation_0]\n",
    "\n",
    "for index in range(len(model_list)):\n",
    "    myTable.add_row([model_list[index],memory_list[index]])\n",
    "    \n",
    "print(myTable)\n",
    "\n",
    "max_value = max(memory_list)\n",
    "max_model = model_list[memory_list.index(max_value)]\n",
    "\n",
    "print(\"Maximum Mixing Probability: \", max_value, \" (\",max_model,\")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probabilities calculated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0p/wklr8kqj0h52vzxckdpz9xhc0000gn/T/ipykernel_716/3255830827.py:32: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  output_final = output_final.drop(used_columns,1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probabilities calculated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0p/wklr8kqj0h52vzxckdpz9xhc0000gn/T/ipykernel_716/3255830827.py:32: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  output_final = output_final.drop(used_columns,1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data joined\n",
      "nullification complete\n",
      "(15419, 24)\n"
     ]
    }
   ],
   "source": [
    "#Step 8: Re-run model using all training data for optimal mixing parameter\n",
    "\n",
    "#Calculate probabilities\n",
    "probability_60_memory = probability_calculation(testdat_final_unique,traindat_final,traindat_purchases,new_product_col,'manhattan',testdat_final)\n",
    "probability_60_demog = probability_calculation(testdat_demog_final_unique,traindat_demog_final,traindat_purchases,demog_col,'manhattan',testdat_demog_final)\n",
    "\n",
    "#Average probabilities\n",
    "probability_avg_60 = 0.6*probability_60_memory + 0.4*probability_60_demog\n",
    "\n",
    "#Generate 1st csv file for averaged probabilities of each customer\n",
    "probability_avg_60.to_csv(\"probabilities_60_avg.csv\",index=False)\n",
    "\n",
    "#Null previous ownership\n",
    "nulled_probability_60 = purchase_nullifier(probability_avg_60[predict_col],testdat_final[new_product_col])\n",
    "\n",
    "#Map to predictions - check dimensions\n",
    "testdat_ncodpers = testdat_use[['fecha_dato','ncodpers']]\n",
    "predictions_output_60 = probabilities_to_predictions(nulled_probability_60,testdat_ncodpers)\n",
    "\n",
    "#Generate 2nd csv file for Top 1 recommended product of each customer\n",
    "predictions_output_60.to_csv('Final Recommendation Result Per Customer.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated probabilities_60_avg_1\n",
      "Generated probabilities_60_avg_2\n",
      "Generated probabilities_60_avg_3\n",
      "Generated probabilities_60_avg_4\n",
      "Generated probabilities_60_avg_5\n",
      "Generated probabilities_60_avg_6\n",
      "Generated probabilities_60_avg_7\n",
      "Generated probabilities_60_avg_8\n",
      "Generated probabilities_60_avg_9\n",
      "Generated probabilities_60_avg_10\n",
      "Generated probabilities_60_avg_11\n",
      "Generated probabilities_60_avg_12\n",
      "Generated probabilities_60_avg_13\n",
      "Generated probabilities_60_avg_14\n",
      "Generated probabilities_60_avg_15\n",
      "Generated probabilities_60_avg_16\n",
      "Generated probabilities_60_avg_17\n",
      "Generated probabilities_60_avg_18\n",
      "Generated probabilities_60_avg_19\n",
      "Generated probabilities_60_avg_20\n",
      "Generated probabilities_60_avg_21\n",
      "Generated probabilities_60_avg_22\n",
      "Generated probabilities_60_avg_23\n",
      "Generated probabilities_60_avg_24\n",
      "Generated probabilities_60_avg_25\n",
      "Generated probabilities_60_avg_26\n",
      "Generated probabilities_60_avg_27\n",
      "Generated probabilities_60_avg_28\n",
      "Generated probabilities_60_avg_29\n",
      "Generated probabilities_60_avg_30\n",
      "Generated probabilities_60_avg_31\n"
     ]
    }
   ],
   "source": [
    "#[Other than modeling] Split the large probability file into multiple files with smaller size in order to upload the result files to Github\n",
    "\n",
    "csvfile = open('probabilities_60_avg.csv', 'r').readlines()\n",
    "filename = \"probabilities_60_avg_\"\n",
    "filecount = 1\n",
    "\n",
    "for i in range(len(csvfile)):\n",
    "    if i % 30000 == 0:\n",
    "        fullfilename = filename + str(filecount)\n",
    "        open(fullfilename + '.csv', 'w+').writelines(csvfile[i:i+30000])\n",
    "        filecount += 1\n",
    "        print(\"Generated \" + fullfilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
